## List of mental models suggested by Super Thinking (written by [Gabriel Weinberg](https://ye.gg))

1. Inverse Thinking:

    “Carl Jacobi was a nineteenth-century German mathematician who often used to say, ‘Invert, always invert’ … He meant that thinking about a problem from an inverse perspective can unlock new solutions and strategies.”

2. Antifragile:

    “Some things benefit from shocks; they thrive and grow when exposed to volatility, randomness, disorder, and stressors and love adventure, risk, and uncertainty. Yet, in spite of the ubiquity of the phenomenon, there is no word for the exact opposite of fragile. Let us call it antifragile. Antifragility is beyond resilience or robustness. The resilient resists shocks and stays the same; the antifragile gets better.” — Nassim Nicholas Taleb

3. Veil of Ignorance:

More about the veil of ignorance: Sloww Sunday Newsletter 083 (Nov 7, 2021) — What is Truth, Ego Development, Meta-Memes, & More

    “Another tactical model to help you have greater empathy is the veil of ignorance, put forth by philosopher John Rawls. It holds that when thinking about how society should be organized, we should do so by imagining ourselves ignorant of our particular place in the world, as if there were a veil preventing us from knowing who we are. Rawls refers to this as the ‘original position.'”

4. Birth Lottery:

Pair with: The Lottery of Birth — All the Things You Don’t Control in Life

    “Speaking of privilege, we (the authors) often say we are lucky to have won the birth lottery. Not only were we not born into slavery, but we were also not born into almost any disadvantaged group. At birth, we were no more deserving of an easier run at life than a child who was born into poverty, or with a disability, or any other type of disadvantage. Yet we are the ones who won this lottery since we do not have these disadvantages.”

5. Just World Hypothesis:

    “It can be challenging to acknowledge that a good portion of your success stems from luck. Many people instead choose to believe that the world is completely fair, orderly, and predictable. This view is called the just world hypothesis, where people always get what they deserve, good or bad, because of their actions alone, with no accounting for luck or randomness. This view is summed up as you reap what you sow.”

6. Learned Helplessness:

    “The problem with the just world hypothesis and victim-blaming is that they make broad judgments about why things are happening to people that are often inaccurate at the individual level. You should also keep in mind that the model of learned helplessness can make it hard for some people to strive for improvement without some assistance. Learned helplessness describes the tendency to stop trying to escape difficult situations because we have gotten used to difficult conditions over time. Someone learns that they are helpless to control their circumstances, so they give up trying to change them.”

7. Thinking Gray:

    “The essence of thinking gray is this: don’t form an opinion about an important matter until you’ve heard all the relevant facts and arguments, or until circumstances force you to form an opinion without recourse to all the facts (which happens occasionally, but much less frequently than one might imagine).” — Steven Sample

8. Five (5) Whys:

Pair with: The Five Whys (or 5 Whys): A Root Cause Analysis for Better Problem Solving

    “One technique commonly used in postmortems is called 5 Whys, where you keep asking the question ‘Why did that happen?’ until you reach the root causes … You can ask as many questions as you need in order to get to the root cause—five is just an arbitrary number.”

9. Tragedy of the Commons:

    “Any shared resource, or commons, is vulnerable to this tragedy … As more and more people make the same decision, the common resource is collectively depleted, reducing the ability for everyone to benefit from it in the future.”

10. Tyranny of Small Decisions:

    “More broadly, the tragedy of the commons arises from what is called the tyranny of small decisions, where a series of small, individually rational decisions ultimately leads to a system-wide negative consequence, or tyranny. It’s death by a thousand cuts.”

11. Free Rider Problem:

    “Another cause of issues like the tragedy of the commons is the free rider problem, where some people get a free ride by using a resource without paying for it.”

12. Externalities:

    “Consequences, good or bad, that affect an entity without its consent, imposed from an external source … Addressing negative externalities is often referred to as internalizing them. Internalizing is an attempt to require the entity that causes the negative externality to pay for it.”

13. Perverse Incentives:

    “High-stakes testing culture—be it for school examinations, job interviews, or professional licensing—creates perverse incentives to ‘teach to the test,’ or worse, cheat.”

14. Boiling Frog:

    “There is a broader class of unintended consequences to similarly watch out for, which also involve making seemingly good short-term decisions that can still add up to a bad outcome in the long term. The mental model often used to describe this class of unintended consequences is called the boiling frog: Suppose a frog jumps into a pot of cold water. Slowly the heat is turned up and up and up, eventually boiling the frog to death. It turns out real frogs generally jump out of the hot water in this situation, but the metaphorical boiling frog persists as a useful mental model describing how a gradual change can be hard to react to, or even perceive.”

15. Path Dependence:

    “The general model for this impact comes from economics and is called path dependence, meaning that the set of decisions, or paths, available to you now is dependent on your past decisions.”

16. Preserving Optionality:

    “Another model from economics offers some reprieve from the limitations of path dependence: preserving optionality. The idea is to make choices that preserve future options … You need to find the right balance between preserving optionality and path dependence.”

17. Reversible Decisions or Irreversible Decisions:

    “There is a natural conflict between the desire to make decisions quickly and the feeling that you need to accumulate more information to be sure you are making the right choice. You can deal with this conflict by categorizing decisions as either reversible decisions or irreversible decisions. Irreversible decisions are hard if not impossible to unwind.”

18. Paradox of Choice:

    “In addition to increased decision-making time, there is evidence that a wealth of options can create anxiety in certain contexts. This anxiety is known as the paradox of choice, named after a 2004 book of the same name by American psychologist Barry Schwartz. Schwartz explains that an overabundance of choice, the fear of making a suboptimal decision, and the potential for lingering regret following missed opportunities can leave people unhappy.”

19. Decision Fatigue:

    “The downside of making many decisions in a limited period: decision fatigue. As you make more and more decisions, you get fatigued, leading to a worsening of decision quality.”

20. Eisenhower Decision Matrix:

    “U.S. President Dwight Eisenhower famously quipped, ‘What is important is seldom urgent and what is urgent is seldom important.’ This quote inspired Stephen Covey in 7 Habits of Highly Effective People to create the Eisenhower Decision Matrix, a two-by-two grid (matrix) that helps you prioritize important activities across both your personal and your professional life by categorizing them according to their urgency and importance.”

21. Bike-Shedding:

    “In his 1957 book Parkinson’s Law, Parkinson presents an example of a budget committee considering an atomic reactor and a bike shed, offering that ‘the time spent on any item of the agenda will be in inverse proportion to the sum involved.’ The committee members are reluctant to deeply discuss all of the complicated aspects of the atomic reactor decision because it is challenging and esoteric. By contrast, everyone wants to weigh in with their opinion on the bike shed decision because it is easy and familiar relative to the reactor, even though it is also relatively unimportant. This phenomenon has become known as bike-shedding.”

22. Heuristic:

    “This is a type of heuristic solution, a trial-and-error solution that is not guaranteed to yield optimal or perfect results, but in many cases is nevertheless very effective. You should consider heuristics because they can be a shortcut to a solution for the problem in front of you, even if they may not work as well in other situations.”

23. Natural Selection:

    “The process that drives biological evolution. It was independently formulated by both Alfred Wallace and Charles Darwin, and later made famous by Darwin’s 1859 book On the Origin of Species by Means of Natural Selection. Traits that provide reproductive advantages are naturally selected over generations, making them more prevalent as species evolve to fit their environment. Beyond biological evolution, natural selection also drives societal evolution, the process by which society changes over time.”

24. Inertia:

    “A physical object’s resistance to changing its current state of motion … Isaac Newton’s first law of motion, often referred to as the law of inertia: ‘An object at rest stays at rest and an object in motion stays in motion with the same speed and in the same direction unless acted upon by an unbalanced force.'”

25. Momentum:

    “Momentum is a model that can help you understand how things change. Momentum and inertia are related concepts. In physics, momentum is the product (multiplication) of mass and velocity, whereas inertia is just a function of mass. That means a heavy object at rest has a lot of inertia since it is hard to move, but it has no momentum since its velocity is zero. However, a heavy object gets momentum quickly once it starts moving. The faster an object goes, the more momentum it has. However, its inertia remains the same (since its mass remains the same), and it is still similarly difficult to change its velocity.”

26. Flywheel:

Pair with: What is the Flywheel Effect by Jim Collins? (& How to Build Momentum for your Life Purpose)

    “The good news is that if you can establish inertia and momentum in an adaptable culture, or in any context really, it can have staying power. A mental model that captures this process well is the flywheel, a rotating physical disk that is used to store energy. Flywheels are still used in many industrial applications, though a more relatable example to get the concept is a children’s merry-go-round. It takes a lot of effort to get a merry-go-round to start spinning, but once it is spinning, it takes little effort to keep it spinning.”

27. Homeostasis:

    “From biology, there is homeostasis, which describes a situation in which an organism constantly regulates itself around a specific target, such as body temperature. When you get too cold, you shiver to warm up; when it’s too hot, you sweat to cool off. In both cases, your body is trying to revert to its normal temperature. That’s helpful, but the same effect also prevents change from the status quo when you want it to occur … When you fight homeostasis—in yourself or in others—look out for the underlying mechanisms that are working against your efforts to make changes.”

28. Activation Energy & Catalyst:

    “Activation energy is the minimum amount of energy needed to activate a chemical reaction between two or more reactants … A catalyst decreases the activation energy needed to start a chemical reaction … More generally, activation energy can refer to the amount of effort it would take to start to change something, and catalyst to anything that would decrease this effort … When attempting change, you want to understand the activation energy required and look for catalysts to make change easier.”

29. Critical Mass & Chain Reaction:

    “In physics, critical mass is the mass of nuclear material needed to create a nuclear chain reaction, where the by-products of one reaction are used as the inputs for the next, chaining them together in a self-perpetuating fashion.”

30. Tipping Point:

    “Critical mass as a super model applies to any system in which an accumulation can reach a threshold amount that causes a major change in the system. The point at which the system starts changing dramatically, rapidly gaining momentum, is often referred to as a tipping point.”

31. Network Effect:

    “Reaching a critical mass is a common proximate cause of a tipping point. But the root cause of why a tipping point has been reached is often found in network effects, where the value of a network grows with each addition to it (the effect).”

32. Entropy:

    “Your luck surface area relates to the natural concept of entropy, which measures the amount of disorder in a system … The natural increase of entropy over time in a closed system is known as the second law of thermodynamics. Thermodynamics is the study of heat. If you consider our universe as the biggest closed system, this law leads to a plausible end state of our universe as a homogenous gas, evenly distributed everywhere, commonly known as the heat death of the universe. On a more practical level, the second law serves as a reminder that orderliness needs to be maintained, lest it be slowly chipped away by disorder. This natural progression is based on the reality that most orderliness doesn’t happen naturally.”

33. Polarity:

Pair with: Polarity Thinking 101: An Introduction to the Power of Polarities (+ Visuals)

    “A concept from physics called polarity, which describes a feature that has only two possible values. A magnet has a north and south pole. An electric charge can be positive or negative … Polarity is useful because it helps you categorize things into one of two states: good or bad, useful or not useful, high-leverage or low-leverage, etc. When you mix two groupings together, you get the 2 × 2 matrix.”

34. Correlation does not imply causation:

    “Just because two events happened in succession, or are correlated, doesn’t mean that the first actually caused the second. Statisticians use the phrase correlation does not imply causation to describe this fallacy.”

35. Maslow’s Hammer:

    “If all you have is a hammer, everything looks like a nail.”

36. Black Swan Events:

    “Extreme, consequential events (that end in things like financial ruin), but which have significantly higher probabilities than you might initially expect.”

37. Systems Thinking:

Pair with: Systems Thinking & Understanding how Everything Connects: “Thinking in Systems” by Donella Meadows (Book Summary)

    “When you attempt to think about the entire system at once. By thinking about the overall system, you are more likely to understand and account for subtle interactions between components that could otherwise lead to unintended consequences from your decisions.”

38. Counterfactual Thinking:

    “Thinking about the past by imagining that the past was different, counter to the facts of what actually occurred.”

39. Lateral Thinking:

    “Thinking that helps you move laterally from one idea to another, as opposed to critical thinking, which is more about judging an idea in front of you. Lateral thinking is thinking outside the box.”

40. Divergent Thinking & Convergent Thinking:

    “(Divergent thinking is) where you actively try to get thinking to diverge in order to discover multiple possible solutions. (Convergent thinking is) where you actively try to get thinking to converge on one solution.”

41. Hysteresis:

    “Describes how a system’s current state can be dependent on its history (what happened previously can impact what will happen next).”

42. Dark Patterns:

    “When influence models are used to manipulate you for someone else’s benefit. The name comes from websites that organize their sites to keep you in the dark through using disguised ads, burying information on hidden costs, or making it really difficult to cancel a subscription or reach support. In short, they use these types of patterns to manipulate and confuse you.”

43. Potemkin Village:

    “Something specifically built to convince people that a situation is better than it actually is.”

44. Slippery Slope Argument:

    “Arguing that one small thing leads to an inevitable chain of events and a terrible final outcome (in the eyes of the person making the argument).”

45. Broken Windows Theory:

    “Proposes that visible evidence of small crimes, for example broken windows in a neighborhood, creates an environment that encourages worse crimes, such as murder.”

46. Deliberate Practice:

    “Deliberately putting people in situations at the limit of their abilities, where they are constantly practicing increasingly difficult skills and receiving consistent real-time feedback.”

47. Simultaneous Invention (or Multiple Discovery):

    “Academic discoveries across the world and similar startups independently emerging simultaneously.”

48. Product/Market Fit:

    “When a product is such a great fit for its market that customers are actively demanding more.”

49. OODA Loop:

    “A decision loop of four steps—observe, orient, decide, act … Take an observing glance at the changing conditions, immediately re-orient their assessment of the situation, decide the next best course of action, act on it without hesitation, and then repeat this loop.”

50. Circle of Competence:

    “The inside of the circle covers areas where you have knowledge or experience—where you are competent—and in those areas, you can think effectively. In areas outside the circle, you cannot. The most dangerous zone is just outside your circle of competence, where you might think you are competent but you really are not.”

### Biases, Fallacies, & Effects

51. Availability Bias:

    “Occurs when a bias, or distortion, creeps into your objective view of reality thanks to information recently made available to you … Availability bias stems from over-reliance on your recent experiences within your frame of reference, at the expense of the big picture.”

52. Self-Serving Bias:

    “You of course tend to view your own behavior in the opposite way, which is called self-serving bias. When you are the actor, you often have self-serving reasons for your behavior, but when you are the observer, you tend to blame the other’s intrinsic nature. (That’s why this model is also sometimes called actor-observer bias.)”

53. Confirmation Bias:

    “Individuals still hang on to old theories in the face of seemingly overwhelming evidence—it happens all the time in science and in life in general. The human tendency to gather and interpret new information in a biased way to confirm preexisting beliefs is called confirmation bias.”

54. Disconfirmation Bias:

    “You may also succumb to holding on to incorrect beliefs because of disconfirmation bias, where you impose a stronger burden of proof on the ideas you don’t want to believe.”

55. Optimistic Probability Bias:

    “Sometimes you may want something to be true so badly that you fool yourself into thinking it is likely to be true. This feeling is known as optimistic probability bias, because you are too optimistic about the probability of success.”

56. Present Bias:

    “One reason why people procrastinate so much is present bias, which is the tendency to overvalue near-term rewards in the present over making incremental progress on long-term goals (see short-termism).”

57. In-Group Favoritism & Out-Group Bias:

    “People are susceptible to the black-and-white fallacy because of the natural tendency to create us versus them framings, thinking that the only two options are ones that either benefit themselves at the expense of ‘others,’ or vice versa. This tendency arises because you often associate identity and self-esteem with group membership, thereafter creating in-group favoritism and, conversely, out-group bias.”

58. Observer-Expectancy Bias (or Experimenter Bias):

    “To take the idea of blinding one step further, the people administering the experiment or analyzing the experiment can also remain unaware of which group the participants are in. This additional blinding helps reduce the impact of observer-expectancy bias (also called experimenter bias), where the cognitive biases of the researchers, or observers, may cause them to influence the outcome in the direction they expected.”

59. Selection Bias:

    “Selection bias can also occur when a sample is selected that is not representative of the broader population of interest, as with online reviews. If the group studied isn’t representative, then the results may not be applicable overall.”

60. Nonresponse Bias:

    “Another type of selection bias, common to surveys, is nonresponse bias, which occurs when a subset of people don’t participate in an experiment after they are selected for it, e.g., they fail to respond to the survey. If the reason for not responding is related to the topic of the survey, the results will end up biased.”

61. Survivorship Bias:

    “Results are biased based on measuring just the population that survived.”

62. Response Bias:

    “One more type of bias that can be inadvertently introduced is response bias. While nonresponse bias is introduced when certain types of people do not respond, for those who do respond, various cognitive biases can cause them to deviate from accurate or truthful responses.”

63. Publication Bias:

    “Unfortunately, studies are much, much more likely to be published if they show statistically significant results, which causes publication bias. Studies that fail to find statistically significant results are still scientifically meaningful, but both researchers and publications have a bias against them for a variety of reasons.”

64. Hindsight Bias:

    “After an event occurs, in hindsight, there is a bias to see it as having been predictable even though there was no real objective basis on which it could have been predicted.”

65. Gambler’s Fallacy:

    “Gambler’s fallacy, named after roulette players who believe that a streak of reds or blacks from a roulette wheel is more likely to end than to continue with the next spin. Suppose you see ten blacks in a row. Those who fall victim to this fallacy expect the next spin to have a higher chance of coming up red, when in fact the underlying probability of each spin hasn’t changed.”

66. Texas Sharpshooter Fallacy:

    “Defining a hypothesis up front helps to avoid the Texas sharpshooter fallacy. This model is named after a joke about a person who comes upon a barn with targets drawn on the side and bullet holes in the middle of each target. He is amazed at the shooter’s accuracy, only to find that the targets were drawn around the bullet holes after the shots were fired. A similar concept is the moving target, where the goal of an experiment is changed to support a desired outcome after seeing the results.”

67. Base Rate Fallacy:

    “When a probability calculation fails to account for the base rate (like the base rate of drunk drivers), the mistake that is made is called the base rate fallacy.”

68. Black-and-White Fallacy:

    “While polarity can be useful, when making comparisons you must be careful to avoid the black-and-white fallacy—thinking that things fall neatly into two groups when they do not. When making decisions, you usually have more than two options. It’s not all black and white. Practically, whenever you are presented with a decision with two options, try to think of more.”

69. Conjunction Fallacy:

    “Most people are, unfortunately, hardwired to latch onto unnecessary assumptions, a predilection called the conjunction fallacy, studied by Amos Tversky and Daniel Kahneman, who provided this example in the October 1983 Psychological Review: ‘Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? 1. Linda is a bank teller. 2. Linda is a bank teller and is active in the feminist movement.’ In their study, most people answered that number 2 is more probable, but that’s impossible unless all bank tellers are also active in the feminist movement. The fallacy arises because the probability of two events in conjunction is always less than or equal to the probability of either one of the events occurring alone.”

70. Sunk-Cost Fallacy:

    “The costs of the project so far, including your time spent, have already been sunk. You can’t get them back. This can be a problem (fallacy) when these previous losses influence you to make a bad decision.”

71. Default Effect:

    “Since many people take the path of least resistance, 401(k) programs also showcase the default effect, the effect stemming from the fact that many people just accept default options.”

72. Backfire Effect:

    “Confirmation bias is so hard to overcome that there is a related model called the backfire effect that describes the phenomenon of digging in further on a position when faced with clear evidence that disproves it. In other words, it often backfires when people try to change your mind with facts and figures, having the opposite effect on you than it should; you become more entrenched in the original, incorrect position, not less.”

73. Cobra Effect:

    “When an attempted solution actually makes the problem worse. This model gets its name from a situation involving actual cobras. When the British were governing India, they were concerned about the number of these deadly snakes, and so they started offering a monetary reward for every snake brought to them. Initially the policy worked well, and the cobra population decreased. But soon, local entrepreneurs started breeding cobras just to collect the bounties. After the government found out and ended the policy, all the cobras that were being used for breeding were released, increasing the cobra population even further.”

74. Streisand Effect:

    “When you unintentionally draw more attention to something when you try to hide it. It’s named for entertainer Barbra Streisand, who sued a photographer and website in 2003 for displaying an aerial photo of her mansion, which she wanted to remain private. Before the suit, the image had been downloaded a total of six times from the site; after people saw news stories about the lawsuit, the site was visited hundreds of thousands of times, and now the photo is free to license and is on display on Wikipedia and many other places. As was said of Watergate, It’s not the crime, it’s the cover-up.”

75. Hydra Effect:

    “Named after the Lernaean Hydra, a beast from Greek mythology that grows two heads for each one that is cut off.”

76. Observer Effect:

    “Where there is an effect on something depending on how you observe it, or even who observes it.”

77. Chilling Effect:

    “When people realized they were being watched by their governments, some of them stopped reading articles that they thought could get them into trouble. The name for this concept is chilling effect … In the legal context where the term chilling effect originated, it refers to when people feel discouraged, or chilled, from freely exercising their rights, for fear of lawsuits or prosecution. More generally, chilling effects are a type of observer effect where the threat of retaliation creates a change in behavior.”

78. Lindy Effect:

    “Inertia in beliefs and behaviors allows entrenched ideas and organizations to persist for long periods of time. The Lindy effect is the name of this phenomenon … The Lindy effect applies to technologies, ideas, organizations, and other nonperishable things. Assuming that the thing in question is not falling out of favor, the longer it endures, the longer you can expect it to further endure.”

79. Bandwagon Effect:

    “Consensus can take hold quickly, as other group members ‘hop on the bandwagon‘ as an idea gains popularity.”

80. Bystander Effect (or Diffusion of Responsibility):

    “Where people fail to take responsibility for something when they are in a group, because they think someone else will take on that responsibility.”

81. Pygmalion Effect:

    “Higher expectations lead to increased performance, as people try to meet the expectations set for them.”

82. Golem Effect:

    “Lower expectations lead to lower performance.”

83. Dunning-Kruger Effect:

    “The confidence people experience over time as they move from being a novice to being an expert (e.g. high confidence at the beginning of the learning curve, then it plummets as you realize everything you don’t know).”

84. Spacing Effect:

    “Learning effects are greater when that learning is spaced out over time, rather than when you study the same amount in a compressed amount of time.”

85. Cognitive Dissonance:

    “The pernicious effects of confirmation bias and related models can be explained by cognitive dissonance, the stress felt by holding two contradictory, dissonant, beliefs at once.”

86. Framing:

    “A frame-of-reference mental trap (or useful trick, depending on your perspective) is framing. Framing refers to the way you present a situation or explanation … Multiple framings can be valid yet convey vastly different perspectives.”

87. Anchoring:

    “Your tendency to rely too heavily on first impressions when making decisions. You get anchored to the first piece of framing information you encounter. This tendency is commonly exploited by businesses when making offers.”

88. Groupthink:

    “Bias that emerges because groups tend to think in harmony. Within group settings, members often strive for consensus, avoiding conflict, controversial issues, or even alternative solutions once it seems a solution is already favored by the group.”

89. Fundamental Attribution Error:

    “The third story, most respectful interpretation, and Hanlon’s razor are all attempts to overcome what psychologists call the fundamental attribution error, where you frequently make errors by attributing others’ behaviors to their internal, or fundamental, motivations rather than external factors. You are guilty of the fundamental attribution error whenever you think someone was mean because she is mean rather than thinking she was just having a bad day.”

90. Semmelweis Reflex:

    “Semmelweis didn’t fully understand the scientific mechanism that underpinned his theory and crafted an initial explanation that turned out to be somewhat incorrect. However, they both noticed obvious and important empirical truths that should have been investigated by other scientists but were reflexively rejected by these scientists because the suggested explanations were not in line with the conventional thinking of the time. Today, this is known as a Semmelweis reflex.”

91. Hyperbolic Discounting:

    “In personal situations, most people discount the future implicitly at relatively high discount rates. And they do so in a manner that is not actually fixed over time, which is called hyperbolic discounting. In other words, people really, really value instant gratification over delayed gratification, and this preference plays a central role in procrastination, along with other areas of life where people struggle with self-control, such as dieting, addiction, etc.”

92. Loss Aversion:

    “You are more inclined to avoid losses, to be averse to them, than you are to want to make similar gains.”

93. Confounding Factor:

    “What is often overlooked when this fallacy arises is a confounding factor, a third, possibly non-obvious factor that influences both the assumed cause and the observed effect, confounding the ability to draw a correct conclusion.”

94. Clustering Illusion:

    “Random data often contains streaks and clusters. Are you surprised to learn that there is a 50 percent chance of getting a run of four heads in a row during any twenty-flip sequence? Streaks like this are often erroneously interpreted as evidence of nonrandom behavior, a failure of intuition called the clustering illusion.”

95. Reciprocity:

    “You tend to feel an obligation to return (or reciprocate) a favor, whether that favor was invited or not.”

96. Commitment:

    “If you agree (or commit) to something, however small, you are more likely to continue to agree later.”

97. Liking:

    “You are more prone to take advice from people you like, and you tend to like people who share characteristics with you.”

98. Social Proof:

    “Drawing on social cues as proof that you are making a good decision. You are more likely to do things that you see other people doing, because of your instinct to want to be part of the group.”

99. Scarcity:

    “How you become more interested in opportunities the less available they are, triggering your fear of missing out (FOMO).”

100. Authority:

    “How you are inclined to follow perceived authority figures.”

### Laws, Principles, & Razors

101. Goodhart’s Law:

    “When a measure becomes a target, it ceases to be a good measure. This more common phrasing is from Cambridge anthropologist Marilyn Strathern in her 1997 paper ‘Improving Ratings’: Audit in the British University System. However, the ‘law‘ is named after English economist Charles Goodhart, whose original formulation in a conference paper presented at the Reserve Bank of Australia in 1975 stated: ‘Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes’ … When you try to incentivize behavior by setting a measurable target, people focus primarily on achieving that measure, often in ways you didn’t intend. Most importantly, their focus on the measure may not correlate to the behavior you hoped to promote.”

102. Hick’s Law:

    “The more choices you have, the harder it is to choose between them. In the early 1950s, psychologists William Hick and Ray Hyman separately conducted a number of experiments to try to quantify the mathematical relationship between the number of choices given and how long it takes to decide. They found that a greater number of choices increased the decision time logarithmically, in a formulation now known as Hick’s law.”

103. Murphy’s Law:

    “Anything that can go wrong, will go wrong. It’s named after aerospace engineer Edward Murphy, from his remarks after his measurement devices failed to perform as expected. It was intended as a defensive suggestion, to remind you to be prepared and to have a plan for when things go wrong.”

104. Sayre’s Law:

    “Sayre’s law, named after political scientist Wallace Sayre, offers that in any dispute the intensity of feeling is inversely proportional to the value of the issues at stake. A related concept is Parkinson’s law of triviality, named after naval historian Cyril Parkinson, which states that organizations tend to give disproportionate weight to trivial issues. Both of these concepts explain how group dynamics can lead the group to focus on the wrong things.”

105. Parkinson’s Law:

    “Work expands so as to fill the time available for its completion.”

106. Hofstadter’s Law:

    “It always takes longer than you expect, even when you take into account Hofstadter’s Law. In other words, things take longer than you expect, even when you consider that they take longer than you expect!”

107. Metcalfe’s Law:

    “This relationship, known as Metcalfe’s law, is named after Robert Metcalfe, the co-inventor of the networking technology Ethernet. It describes the nonlinear growth in network value when nodes are connected to one another.”

108. Joy’s Law:

    “No matter who you are, most of the smartest people work for someone else … Great people are unlikely to be concentrated in a single organization.”

109. Law of Diminishing Returns:

    “After you determine the 80/20 and address the low-hanging fruit, each additional hour of work will unfortunately produce less and less impactful results. In economics, this model is called the law of diminishing returns. It is the tendency for continued effort to diminish in effectiveness after a certain level of result has been achieved.”

110. Law of Diminishing Utility:

    “There is a similar concept called the law of diminishing utility, which says that the value, or utility, of consuming an additional item is usually, after a certain point, less than the value of the previous one consumed.”

111. First Principles:

    “Thinking from the bottom up, using basic building blocks of what you think is true to build sound (and sometimes new) conclusions. First principles are the group of self-evident assumptions that make up the foundation on which your conclusions rest—the ingredients in a recipe or the mathematical axioms that underpin a formula … When arguing from first principles, you are deliberately starting from scratch.”

112. Precautionary Principle:

    “When an action could possibly create harm of an unknown magnitude, you should proceed with extreme caution before enacting the policy.”

113. Pareto Principle:

    “The Pareto principle can help you find high-leverage activities. It states that in many situations, 80 percent of the results come from approximately 20 percent of the effort. Addressing this 20 percent is therefore a high-leverage activity.”

114. Shirky Principle:

    “A model related to the strategy tax is the Shirky principle, named after economics writer Clay Shirky. The Shirky principle states, institutions will try to preserve the problem to which they are the solution.”

115. Chatelier’s Principle:

    “When any chemical system at equilibrium is subject to a change in conditions, such as a shift in temperature, volume, or pressure, it readjusts itself into a new equilibrium state and usually partially counteracts the change.”

116. Peter Principle:

    “Managers rise to the level of their incompetence.”

117. Ockham’s Razor:

    “The simplest explanation is most likely to be true. When you encounter competing explanations that plausibly explain a set of data equally well, you probably want to choose the simplest one to investigate first … This model is a razor because it ‘shaves off’ unnecessary assumptions.”

118. Hanlon’s Razor:

    “Never attribute to malice that which is adequately explained by carelessness. Like Ockham’s razor, Hanlon’s razor seeks out the simplest explanation. And when people do something harmful, the simplest explanation is usually that they took the path of least resistance. That is, they carelessly created the negative outcome; they did not cause the outcome out of malice.”

